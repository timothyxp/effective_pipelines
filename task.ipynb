{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76efa72",
   "metadata": {},
   "source": [
    "# Week 5: home assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b8117",
   "metadata": {},
   "source": [
    "## Assignment structure\n",
    "\n",
    "- DIY: loss scaling (3 points)\n",
    "- Efficient batching for language modelling (5 points)\n",
    "- Profiling of the pipeline (2 points)\n",
    "\n",
    "Your grade for the assignment is the sum of the points for the sections above. Maximum is 10 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b03ad",
   "metadata": {},
   "source": [
    "## DIY: loss scaling (1.5 + 1.5 points)\n",
    "\n",
    "Let's use a semantic segmentation pipeline for this section. Your task is to train the model in the AMP mode with loss scaler implemented by you. You **can use** `torch.cuda.amp.autocast` and you **cannot use** `torch.cuda.amp.GradScaler()` (you may only for checking your solution).\n",
    "\n",
    "Let us remind what loss scaling is. Loss scaling is used to avoid the gradient underflow problem, when computing gradients in FP16 precision. The issue here is that while training in full precision, we might acquire rather small values in the gradients, which will vanish when we cast a tensor to a half precision. To fix the problem the following solution is used:\n",
    "\n",
    "- make a forward pass for the model and compute the loss\n",
    "- multiply loss value to some factor\n",
    "- call `.backward()`\n",
    "- update model's master weights with **unscaled** FP32 gradients\n",
    "\n",
    "**Note.** Loss scaling might be done in two different ways: static and dynamic ones. In static mode, you choose a factor for scaling only once and use it for the whole training procedure. In dynamic mode you recompute the factor each time you scale the loss. \n",
    "\n",
    "For static scaling you will get **1.5 points**, for dynamic scaling you will get additional **1.5 points**. The task is done if you managed to stably achieve high accuracy values (0.985+) within 5 training epochs. For a start, you can run the training in a full precision mode, then try to run in an AMP mode with and without PyTorch loss scaler. You will observe that adding a scaler gives you additional accuracy points.\n",
    "\n",
    "**Hint.** To make sure that you're doing everything right, you might want to examine gradients' values: (almost) no zeros must be present there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d63306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-07 13:47:38--  https://www.dropbox.com/s/tc1qo73rrm3gt3m/CARVANA.zip\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.70.18, 2620:100:6026:18::a27d:4612\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.70.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/tc1qo73rrm3gt3m/CARVANA.zip [following]\n",
      "--2022-03-07 13:47:38--  https://www.dropbox.com/s/raw/tc1qo73rrm3gt3m/CARVANA.zip\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucbe7aeba3b62c4d346ece644953.dl.dropboxusercontent.com/cd/0/inline/BhAv_omEPaOc0AnpPe6-LnXmrTrkSKTiXlde0ulfzDxpmSZ516DVmzaadVIDfjED7GbSuJCTRiLaB7Wtk1P3sTnc03CL6ZQHCuMu991StcrEMPVYEQR1VUBi5tbfkGBWtxKvnfDVSkgFnpRfK2ojK7d_/file# [following]\n",
      "--2022-03-07 13:47:39--  https://ucbe7aeba3b62c4d346ece644953.dl.dropboxusercontent.com/cd/0/inline/BhAv_omEPaOc0AnpPe6-LnXmrTrkSKTiXlde0ulfzDxpmSZ516DVmzaadVIDfjED7GbSuJCTRiLaB7Wtk1P3sTnc03CL6ZQHCuMu991StcrEMPVYEQR1VUBi5tbfkGBWtxKvnfDVSkgFnpRfK2ojK7d_/file\n",
      "Resolving ucbe7aeba3b62c4d346ece644953.dl.dropboxusercontent.com (ucbe7aeba3b62c4d346ece644953.dl.dropboxusercontent.com)... 162.125.70.15, 2620:100:6026:15::a27d:460f\n",
      "Connecting to ucbe7aeba3b62c4d346ece644953.dl.dropboxusercontent.com (ucbe7aeba3b62c4d346ece644953.dl.dropboxusercontent.com)|162.125.70.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/BhB3nOvmHoUmjXPPCcGraqRERPQE0sFQsQbPa7s3ql7QwF7M2Z417_-X-pR0z-FCZSNvuC-fR2AUqj9Q_AbziY3TDZLO0kPkTtbi5a2Pr4A5lV73ll6IxcWu7pVtq2_jmqOKGjuf4HDJfk8EA6Gl_nA1MyixXcGyI1L6xSRT_rXdsYdXdUo51g1rrEL4-aFBsnT9UyT5V8vnmLQ8ZpURGiByhYMGoBHxGAsG9uhMI-aa4Ae22iCdFbDx34eQMMVZbkfYrNt6h5fnMTLDqiJv-D3P3uOb6pTLxELPKNX97aL6m06fczmQlgt5xewVcxFYqct_P1ZRj3B9oZd3pnYDnO_Z9y7QBhQt5o_6Zc1MyE1pEY_fF2NP7cVtWSZ-WitXGTA/file [following]\n",
      "--2022-03-07 13:47:39--  https://ucbe7aeba3b62c4d346ece644953.dl.dropboxusercontent.com/cd/0/inline2/BhB3nOvmHoUmjXPPCcGraqRERPQE0sFQsQbPa7s3ql7QwF7M2Z417_-X-pR0z-FCZSNvuC-fR2AUqj9Q_AbziY3TDZLO0kPkTtbi5a2Pr4A5lV73ll6IxcWu7pVtq2_jmqOKGjuf4HDJfk8EA6Gl_nA1MyixXcGyI1L6xSRT_rXdsYdXdUo51g1rrEL4-aFBsnT9UyT5V8vnmLQ8ZpURGiByhYMGoBHxGAsG9uhMI-aa4Ae22iCdFbDx34eQMMVZbkfYrNt6h5fnMTLDqiJv-D3P3uOb6pTLxELPKNX97aL6m06fczmQlgt5xewVcxFYqct_P1ZRj3B9oZd3pnYDnO_Z9y7QBhQt5o_6Zc1MyE1pEY_fF2NP7cVtWSZ-WitXGTA/file\n",
      "Reusing existing connection to ucbe7aeba3b62c4d346ece644953.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 461489772 (440M) [application/zip]\n",
      "Saving to: ‘CARVANA.zip’\n",
      "\n",
      "CARVANA.zip         100%[===================>] 440.11M  13.7MB/s    in 35s     \n",
      "\n",
      "2022-03-07 13:48:16 (12.4 MB/s) - ‘CARVANA.zip’ saved [461489772/461489772]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download and unpack data\n",
    "!wget https://www.dropbox.com/s/tc1qo73rrm3gt3m/CARVANA.zip  # Carvana dataset\n",
    "!unzip -q CARVANA.zip\n",
    "!rm -rf ./train/.DS_Store\n",
    "!rm -rf ./train_masks/.DS_Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a4dde",
   "metadata": {},
   "source": [
    "## Efficient batching for language modelling (1 + 1 + 3 points)\n",
    "\n",
    "In this part we suggest you examine the efficiency of the three batching approaches we discussed during the seminar. Let us remind you shortly:\n",
    "\n",
    "**BRAIN**: pad everything to a fixed `max_length`\n",
    "\n",
    "**BIG BRAIN**: pad only in the `collate_fn`\n",
    "\n",
    "**ULTRA DUPER BIG BRAIN**: presort data to sample sequences smartly, preserving similar examples length in the batch\n",
    "\n",
    "___\n",
    "More formally, we suggest you download [WikiText-103 dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) and implement all of the mentioned approaches. Use the training part for all of the task's sub-problems.\n",
    "\n",
    "- For naive batching, you will need to implement a Pytorch Dataset class that will parse training data from the source files of the dataset and pad everything to a `max_length=640` of the training samples. For sequences longer than 640 tokens just truncate the overflowing part. **(1 point)**\n",
    "- For the second approach, you will need to implement the approach from the seminar for this dataset. More specifically, you needed to pad sequences only up to maximum sample length in the current batch. **(1 point)**\n",
    "- Finally, for the third approach, you will need to make a small trick. While initializing the dataset, you need to split it into the several bins (let's say, python lists) by samples length. For the task we suggest you uniformly split the samples list sorted by sample length. Conduct experiments for 1, 5, 10, 25, 50 bins. While calling a `__getitem__` method, you firstly sample a bin number, then sample the needed examples number form the bin and pad them with collator from the second subtask. **(3 points)**\n",
    "\n",
    "For each of the implemented methods mock one training epoch and provide min, max, mean and median batch processing times. Use a `pandas.DataFrame` to display the results in the notebook. For mocking a training epoch we suggest you construct a small GPT-2-like model: use `nn.Embedding` layer, `PositionalEncoding` class from `transformer.py` file and a single `nn.TransformerDecoder` layer with hidden size 1024 and 8 heads. For tokenization use `torchtext.data.utils.get_tokenizer(\"basic_english\")`. Run one epoch **without a backward pass**. Make sure you've [warmed up](https://forums.developer.nvidia.com/t/why-warm-up/48565) GPU before computing the statistics and do not forget about asynchronous CUDA kernels execution.\n",
    "\n",
    "**Note.** In the third subtask you might want to use (not obligatory) a `batch_sampler` in the data loader. For that, you need to inspect the corresponding Pytorch docs [section](https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2746660",
   "metadata": {},
   "source": [
    "## Profiling (2 points)\n",
    "\n",
    "In this section, you're given a training script for a Transformer model on WikiText2 dataset. Your task is to examine the bottlenecks of the model. You can find the model script in the `transformer.py` file. As you might notice, this is a PyTorch Transformer implementation.\n",
    "\n",
    "We suppose that in this task you use [PyTorch profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html). However, feel free to use any other profiler that we've discussed. In the training function, you can vary the number of steps that are done during one epoch. We suggest you use only one epoch since our goal is not to train a model but to profile its performance.\n",
    "\n",
    "To complete the task, provide a detailed description of the model performance:\n",
    "- Forward pass\n",
    "    - Inspect PositionalEncoding layer\n",
    "    - Inspect the Embedding layer\n",
    "    - Inspect Attention layer (both self attention and projections computations)\n",
    "- Backward pass\n",
    "    - How long does it take compared to a forward pass?\n",
    "    \n",
    "Provide corresponding profiler's outputs and analyse them. We assume that you will analyse all of the mentioned model parts and other parts if you think it is reasonable (their time consumption is comparable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb2d6cf-3694-47a9-96b2-34e695d1ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef310df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "# code sourse: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from tqdm.auto import trange\n",
    "\n",
    "from transformer import generate_square_subsequent_mask, TransformerModel\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c248c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(split=\"train\")\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1285366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc06e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5964f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    i = 0\n",
    "    for batch in trange(0, train_data.size(0) - 1, bptt, desc=\"Epoch progress: \"):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        with record_function(\"forward\"):\n",
    "            output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        # feel free to comment out this \n",
    "        optimizer.zero_grad()\n",
    "        with record_function(\"backward\"):\n",
    "            loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                  f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                  f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\")\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        i += 1\n",
    "            \n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64160a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c6076009ee4ef0a332eab7ab213447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch progress:   0%|          | 0/2929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 24.74 | loss  1.53 | ppl     4.62\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch  4.31 | loss  0.68 | ppl     1.97\n",
      "| epoch   1 |  4200/ 2928 batches | lr 5.00 | ms/batch  4.51 | loss  0.46 | ppl     1.58\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 1\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print(\"-\" * 89)\n",
    "    print(f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "          f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\")\n",
    "    print(\"-\" * 89)\n",
    "\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         best_model = copy.deepcopy(model)\n",
    "\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da9156d-7268-40f0-9538-76aa01d78270",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = prof.key_averages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93546048-bf8c-48d1-ae1f-67a136108d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               backward        29.75%       27.478s        29.95%       27.663s       9.445ms       0.000us         0.00%       3.012ms       1.028us          2929  \n",
      "                                                forward         0.47%     435.287ms        26.12%       24.125s       8.237ms       0.000us         0.00%        3.803s       1.298ms          2929  \n",
      "                                            transformer         0.47%     432.353ms        19.21%       17.738s       6.056ms       0.000us         0.00%        1.252s     427.535us          2929  \n",
      "                                       cudaLaunchKernel        17.69%       16.342s        17.69%       16.342s      18.294us       0.000us         0.00%       0.000us       0.000us        893291  \n",
      "                                         self_attention         2.40%        2.214s        10.08%        9.308s       1.589ms       0.000us         0.00%     743.252ms     126.878us          5858  \n",
      "                                        attn_projection         1.65%        1.528s         8.49%        7.842s       1.339ms       0.000us         0.00%     508.997ms      86.889us          5858  \n",
      "                                             aten::add_         2.69%        2.482s         5.31%        4.901s      25.746us        1.957s         8.83%        1.957s      10.279us        190358  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.48%     446.109ms         5.17%        4.775s     181.157us       0.000us         0.00%        6.300s     239.005us         26361  \n",
      "                                               encoding         0.26%     240.321ms         4.97%        4.586s       1.566ms       0.000us         0.00%      20.719ms       7.074us          2929  \n",
      "                                               aten::mm         2.64%        2.441s         4.70%        4.340s      54.880us        8.615s        38.89%        8.615s     108.940us         79083  \n",
      "                                            MmBackward0         0.62%     568.741ms         4.69%        4.329s     164.234us       0.000us         0.00%        6.300s     239.005us         26361  \n",
      "                                           aten::linear         0.36%     330.321ms         4.63%        4.279s     162.312us       0.000us         0.00%        3.014s     114.329us         26361  \n",
      "                                        aten::embedding         0.08%      77.639ms         4.53%        4.183s       1.428ms       0.000us         0.00%      14.519ms       4.957us          2929  \n",
      "                                             aten::norm         2.74%        2.529s         4.41%        4.075s      49.689us     738.142ms         3.33%     738.142ms       9.000us         82012  \n",
      "                                     aten::index_select         0.13%     118.188ms         4.41%        4.071s       1.390ms      14.519ms         0.07%      14.519ms       4.957us          2929  \n",
      "autograd::engine::evaluate_function: torch::autograd...         0.78%     724.395ms         3.64%        3.361s      42.505us       0.000us         0.00%     628.076ms       7.942us         79083  \n",
      "                                        aten::transpose         3.13%        2.889s         3.53%        3.257s      15.234us       0.000us         0.00%       0.000us       0.000us        213817  \n",
      "                                          aten::dropout         0.19%     174.353ms         3.02%        2.786s     105.689us       0.000us         0.00%      78.232ms       2.968us         26361  \n",
      "                        torch::autograd::AccumulateGrad         0.58%     534.973ms         2.86%        2.637s      33.346us       0.000us         0.00%     628.076ms       7.942us         79083  \n",
      "                      Optimizer.zero_grad#SGD.zero_grad         0.67%     618.679ms         2.85%        2.628s     897.121us       0.000us         0.00%     281.548ms      96.124us          2929  \n",
      "                                   aten::_fused_dropout         1.20%        1.112s         2.83%        2.612s      99.075us      78.232ms         0.35%      78.232ms       2.968us         26361  \n",
      "                                Optimizer.step#SGD.step         0.99%     916.311ms         2.76%        2.546s     869.336us       0.000us         0.00%     610.613ms     208.471us          2929  \n",
      "                                             aten::item         0.02%      19.325ms         2.72%        2.510s     857.004us       0.000us         0.00%       3.384ms       1.155us          2929  \n",
      "     autograd::engine::evaluate_function: ViewBackward0         0.61%     567.824ms         2.70%        2.492s      47.268us       0.000us         0.00%     115.774ms       2.196us         52722  \n",
      "                              aten::_local_scalar_dense         0.04%      33.656ms         2.70%        2.491s     850.406us       3.384ms         0.02%       3.384ms       1.155us          2929  \n",
      "                                           aten::matmul         0.51%     467.097ms         2.69%        2.481s      94.100us       0.000us         0.00%        2.315s      87.815us         26361  \n",
      "                                        cudaMemcpyAsync         2.64%        2.442s         2.64%        2.442s     825.710us       0.000us         0.00%       0.000us       0.000us          2957  \n",
      "                                            aten::zero_         0.70%     641.939ms         2.49%        2.301s      19.648us       0.000us         0.00%     767.038ms       6.548us        117133  \n",
      "                                            aten::clone         0.45%     419.593ms         2.29%        2.117s      60.238us       0.000us         0.00%     180.869ms       5.146us         35148  \n",
      "                                       aten::empty_like         0.90%     835.670ms         2.21%        2.041s      17.423us       0.000us         0.00%       0.000us       0.000us        117160  \n",
      "autograd::engine::evaluate_function: TransposeBackwa...         0.20%     189.252ms         2.20%        2.030s      69.298us       0.000us         0.00%       0.000us       0.000us         29290  \n",
      "      autograd::engine::evaluate_function: AddBackward0         0.76%     701.309ms         2.17%        2.005s      42.775us       0.000us         0.00%        2.180s      46.523us         46864  \n",
      "                                             aten::mul_         0.99%     915.267ms         2.09%        1.931s      24.422us     517.155ms         2.33%     517.155ms       6.539us         79083  \n",
      "                                     TransposeBackward0         0.11%     104.133ms         1.99%        1.840s      62.837us       0.000us         0.00%       0.000us       0.000us         29290  \n",
      "                                                aten::t         0.94%     866.950ms         1.97%        1.821s      13.818us       0.000us         0.00%       0.000us       0.000us        131805  \n",
      "autograd::engine::evaluate_function: FusedDropoutBac...         0.32%     293.519ms         1.95%        1.803s      68.407us       0.000us         0.00%      53.958ms       2.047us         26361  \n",
      "                                       aten::layer_norm         0.08%      77.019ms         1.95%        1.799s     153.561us       0.000us         0.00%     185.345ms      15.820us         11716  \n",
      "autograd::engine::evaluate_function: NativeLayerNorm...         0.23%     214.260ms         1.94%        1.796s     153.285us       0.000us         0.00%     325.303ms      27.766us         11716  \n",
      "                                            aten::fill_         0.78%     719.865ms         1.89%        1.746s      19.875us     770.050ms         3.48%     770.050ms       8.766us         87844  \n",
      "                                aten::native_layer_norm         1.21%        1.120s         1.86%        1.722s     146.987us     185.345ms         0.84%     185.345ms      15.820us         11716  \n",
      "                                            aten::empty         1.82%        1.684s         1.83%        1.686s       5.757us       0.000us         0.00%       0.000us       0.000us        292902  \n",
      "                                          aten::reshape         0.63%     579.179ms         1.77%        1.635s      19.249us       0.000us         0.00%      84.231ms       0.992us         84941  \n",
      "                                              aten::bmm         0.86%     797.500ms         1.74%        1.609s      45.786us        1.354s         6.11%        1.354s      38.529us         35148  \n",
      "                               NativeLayerNormBackward0         0.15%     141.582ms         1.71%        1.582s     134.997us       0.000us         0.00%     325.303ms      27.766us         11716  \n",
      "      autograd::engine::evaluate_function: BmmBackward0         0.21%     197.354ms         1.71%        1.581s     134.908us       0.000us         0.00%        1.125s      96.055us         11716  \n",
      "                                       aten::contiguous         0.10%      89.954ms         1.69%        1.559s      66.554us       0.000us         0.00%      96.638ms       4.124us         23432  \n",
      "                                          ViewBackward0         0.26%     238.258ms         1.65%        1.528s      28.978us       0.000us         0.00%      84.231ms       1.598us         52722  \n",
      "                                  FusedDropoutBackward0         0.19%     179.284ms         1.63%        1.510s      57.273us       0.000us         0.00%      53.958ms       2.047us         26361  \n",
      "                                              aten::sum         0.97%     897.487ms         1.60%        1.477s      45.836us        2.208s         9.97%        2.208s      68.521us         32219  \n",
      "                       aten::native_layer_norm_backward         0.63%     585.471ms         1.56%        1.440s     122.912us     325.303ms         1.47%     325.303ms      27.766us         11716  \n",
      "                                           BmmBackward0         0.22%     202.185ms         1.50%        1.383s     118.063us       0.000us         0.00%        1.125s      96.055us         11716  \n",
      "                                    aten::_masked_scale         0.56%     513.135ms         1.44%        1.330s      50.472us      53.958ms         0.24%      53.958ms       2.047us         26361  \n",
      "                                            aten::copy_         0.58%     532.953ms         1.25%        1.152s      32.748us     181.026ms         0.82%     181.026ms       5.146us         35177  \n",
      "                                              aten::add         0.74%     685.758ms         1.24%        1.142s      38.984us      88.310ms         0.40%      88.310ms       3.015us         29290  \n",
      "                                           aten::detach         0.52%     483.186ms         0.98%     904.815ms       5.721us       0.000us         0.00%       0.000us       0.000us        158166  \n",
      "                                              aten::div         0.55%     503.910ms         0.86%     790.009ms      44.953us      32.814ms         0.15%      32.814ms       1.867us         17574  \n",
      "                                    positional_encoding         0.29%     263.929ms         0.82%     760.053ms     259.492us       0.000us         0.00%      34.404ms      11.746us          2929  \n",
      "                                            aten::stack         0.22%     204.096ms         0.82%     757.881ms     258.751us       0.000us         0.00%      26.884ms       9.179us          2929  \n",
      "                                             aten::view         0.78%     719.656ms         0.78%     719.656ms       4.725us       0.000us         0.00%       0.000us       0.000us        152308  \n",
      "                                    aten::empty_strided         0.75%     689.912ms         0.75%     690.406ms      12.400us       0.000us         0.00%       0.000us       0.000us         55680  \n",
      "                                       aten::as_strided         0.70%     645.569ms         0.70%     645.569ms       1.563us       0.000us         0.00%       0.000us       0.000us        412989  \n",
      "        autograd::engine::evaluate_function: TBackward0         0.21%     190.368ms         0.68%     625.482ms      23.728us       0.000us         0.00%       0.000us       0.000us         26361  \n",
      "autograd::engine::evaluate_function: UnsafeViewBackw...         0.22%     205.241ms         0.66%     612.402ms      23.231us       0.000us         0.00%       0.000us       0.000us         26361  \n",
      "                                              aten::cat         0.08%      73.505ms         0.64%     590.792ms      67.235us       0.000us         0.00%     110.712ms      12.600us          8787  \n",
      "                                            aten::zeros         0.35%     321.262ms         0.62%     576.894ms      16.413us       0.000us         0.00%     117.673ms       3.348us         35148  \n",
      "                                              aten::mul         0.33%     302.758ms         0.57%     526.818ms      35.970us      28.129ms         0.13%      28.129ms       1.921us         14646  \n",
      "                                             aten::_cat         0.26%     239.077ms         0.56%     517.287ms      58.870us     110.712ms         0.50%     110.712ms      12.600us          8787  \n",
      "autograd::engine::evaluate_function: SoftmaxBackward...         0.09%      84.523ms         0.54%     500.908ms      85.508us       0.000us         0.00%      24.641ms       4.206us          5858  \n",
      "    autograd::engine::evaluate_function: SplitBackward0         0.08%      74.564ms         0.52%     480.646ms      82.050us       0.000us         0.00%      83.828ms      14.310us          5858  \n",
      "                                             TBackward0         0.10%      92.502ms         0.47%     435.114ms      16.506us       0.000us         0.00%       0.000us       0.000us         26361  \n",
      "autograd::engine::evaluate_function: NllLossBackward...         0.09%      82.126ms         0.47%     430.371ms     146.934us       0.000us         0.00%     407.874ms     139.254us          2929  \n",
      "                                                 detach         0.46%     421.629ms         0.46%     422.452ms       2.671us       0.000us         0.00%       0.000us       0.000us        158166  \n",
      "                                       SoftmaxBackward0         0.06%      52.679ms         0.45%     416.385ms      71.080us       0.000us         0.00%      24.641ms       4.206us          5858  \n",
      "                               aten::cross_entropy_loss         0.04%      36.862ms         0.45%     411.241ms     140.403us       0.000us         0.00%        1.334s     455.465us          2929  \n",
      "                                    UnsafeViewBackward0         0.13%     120.526ms         0.44%     407.161ms      15.446us       0.000us         0.00%       0.000us       0.000us         26361  \n",
      "                                         SplitBackward0         0.03%      31.397ms         0.44%     406.082ms      69.321us       0.000us         0.00%      83.828ms      14.310us          5858  \n",
      "                                     aten::_unsafe_view         0.32%     294.686ms         0.43%     400.017ms      10.505us       0.000us         0.00%       0.000us       0.000us         38077  \n",
      "                                        aten::unsqueeze         0.34%     309.991ms         0.42%     391.058ms       4.604us       0.000us         0.00%       0.000us       0.000us         84941  \n",
      "                           aten::_softmax_backward_data         0.15%     139.321ms         0.39%     363.706ms      62.087us      12.067ms         0.05%      24.641ms       4.206us          5858  \n",
      "                                       NllLossBackward0         0.05%      45.347ms         0.38%     348.245ms     118.896us       0.000us         0.00%     407.874ms     139.254us          2929  \n",
      "autograd::engine::evaluate_function: EmbeddingBackwa...         0.03%      29.793ms         0.37%     340.399ms     116.217us       0.000us         0.00%     230.077ms      78.551us          2929  \n",
      "      autograd::engine::evaluate_function: DivBackward0         0.06%      58.308ms         0.34%     317.864ms      54.262us       0.000us         0.00%      12.453ms       2.126us          5858  \n",
      "                                            aten::chunk         0.03%      27.325ms         0.34%     317.769ms      54.245us       0.000us         0.00%       0.000us       0.000us          5858  \n",
      "                                   aten::_reshape_alias         0.34%     316.601ms         0.34%     316.601ms       4.324us       0.000us         0.00%       0.000us       0.000us         73225  \n",
      "     autograd::engine::evaluate_function: GeluBackward0         0.09%      78.704ms         0.34%     311.074ms      53.102us       0.000us         0.00%      43.758ms       7.470us          5858  \n",
      "                                     EmbeddingBackward0         0.02%      19.362ms         0.34%     310.606ms     106.045us       0.000us         0.00%     230.077ms      78.551us          2929  \n",
      "                                aten::nll_loss_backward         0.10%      89.961ms         0.33%     302.898ms     103.413us      40.057ms         0.18%     407.874ms     139.254us          2929  \n",
      "                                          aten::softmax         0.05%      45.433ms         0.32%     295.995ms      50.528us       0.000us         0.00%      29.449ms       5.027us          5858  \n",
      "                               aten::embedding_backward         0.04%      36.140ms         0.32%     291.244ms      99.435us       0.000us         0.00%     230.077ms      78.551us          2929  \n",
      "                                            aten::split         0.11%     103.298ms         0.31%     290.444ms      49.581us       0.000us         0.00%       0.000us       0.000us          5858  \n",
      "                                           DivBackward0         0.05%      41.588ms         0.28%     259.556ms      44.308us       0.000us         0.00%      12.453ms       2.126us          5858  \n",
      "                         aten::embedding_dense_backward         0.07%      69.016ms         0.28%     255.104ms      87.096us     112.404ms         0.51%     230.077ms      78.551us          2929  \n",
      "                                         aten::_softmax         0.18%     163.965ms         0.27%     250.562ms      42.773us      29.449ms         0.13%      29.449ms       5.027us          5858  \n",
      "                                             aten::gelu         0.17%     161.301ms         0.27%     248.713ms      42.457us      29.444ms         0.13%      29.444ms       5.026us          5858  \n",
      "                                          GeluBackward0         0.04%      38.043ms         0.25%     232.370ms      39.667us       0.000us         0.00%      43.758ms       7.470us          5858  \n",
      "                                            aten::slice         0.19%     179.754ms         0.24%     225.940ms       8.571us       0.000us         0.00%       0.000us       0.000us         26361  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.23%     208.654ms         0.23%     208.654ms       2.544us       0.000us         0.00%       0.000us       0.000us         82012  \n",
      "autograd::engine::evaluate_function: LogSoftmaxBackw...         0.06%      58.639ms         0.22%     202.456ms      69.121us       0.000us         0.00%        3.281s       1.120ms          2929  \n",
      "                                    aten::gelu_backward         0.12%     110.550ms         0.21%     194.327ms      33.173us      43.758ms         0.20%      43.758ms       7.470us          5858  \n",
      "                                      aten::nll_loss_nd         0.02%      16.999ms         0.21%     193.130ms      65.937us       0.000us         0.00%      72.021ms      24.589us          2929  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 92.358s\n",
      "Self CUDA time total: 22.153s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(stat.table(sort_by=\"cpu_time_total\", row_limit=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5a135d-8987-45b3-b31d-d4f0cd687dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf82485-7adc-49ec-aa6f-4bbb37a7b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               aten::mm         2.64%        2.441s         4.70%        4.340s      54.880us        8.615s        38.89%        8.615s     108.940us         79083  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.48%     446.109ms         5.17%        4.775s     181.157us       0.000us         0.00%        6.300s     239.005us         26361  \n",
      "                                            MmBackward0         0.62%     568.741ms         4.69%        4.329s     164.234us       0.000us         0.00%        6.300s     239.005us         26361  \n",
      "                                                forward         0.47%     435.287ms        26.12%       24.125s       8.237ms       0.000us         0.00%        3.803s       1.298ms          2929  \n",
      "autograd::engine::evaluate_function: LogSoftmaxBackw...         0.06%      58.639ms         0.22%     202.456ms      69.121us       0.000us         0.00%        3.281s       1.120ms          2929  \n",
      "                                    LogSoftmaxBackward0         0.04%      40.968ms         0.16%     143.817ms      49.101us       0.000us         0.00%        3.281s       1.120ms          2929  \n",
      "                       aten::_log_softmax_backward_data         0.06%      53.871ms         0.11%     102.849ms      35.114us        3.281s        14.81%        3.281s       1.120ms          2929  \n",
      "void at::native::(anonymous namespace)::cunn_SoftMax...         0.00%       0.000us         0.00%       0.000us       0.000us        3.281s        14.81%        3.281s       1.120ms          2929  \n",
      "                                           aten::linear         0.36%     330.321ms         4.63%        4.279s     162.312us       0.000us         0.00%        3.014s     114.329us         26361  \n",
      "                                  volta_sgemm_32x128_nn         0.00%       0.000us         0.00%       0.000us       0.000us        2.974s        13.43%        2.974s     338.492us          8787  \n",
      "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us        2.593s        11.71%        2.593s     177.076us         14645  \n",
      "                                  volta_sgemm_32x128_nt         0.00%       0.000us         0.00%       0.000us       0.000us        2.334s        10.53%        2.334s     265.585us          8787  \n",
      "                                           aten::matmul         0.51%     467.097ms         2.69%        2.481s      94.100us       0.000us         0.00%        2.315s      87.815us         26361  \n",
      "                                              aten::sum         0.97%     897.487ms         1.60%        1.477s      45.836us        2.208s         9.97%        2.208s      68.521us         32219  \n",
      "      autograd::engine::evaluate_function: AddBackward0         0.76%     701.309ms         2.17%        2.005s      42.775us       0.000us         0.00%        2.180s      46.523us         46864  \n",
      "                                             aten::add_         2.69%        2.482s         5.31%        4.901s      25.746us        1.957s         8.83%        1.957s      10.279us        190358  \n",
      "void at::native::reduce_kernel<256, 2, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us        1.575s         7.11%        1.575s     537.788us          2929  \n",
      "                                              aten::bmm         0.86%     797.500ms         1.74%        1.609s      45.786us        1.354s         6.11%        1.354s      38.529us         35148  \n",
      "                               aten::cross_entropy_loss         0.04%      36.862ms         0.45%     411.241ms     140.403us       0.000us         0.00%        1.334s     455.465us          2929  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us        1.301s         5.87%        1.301s       7.166us        181571  \n",
      "                                      aten::log_softmax         0.04%      36.313ms         0.20%     181.249ms      61.881us       0.000us         0.00%        1.262s     430.876us          2929  \n",
      "                                     aten::_log_softmax         0.10%      88.235ms         0.15%     142.642ms      48.700us        1.262s         5.70%        1.262s     430.876us          2929  \n",
      "void at::native::(anonymous namespace)::cunn_SoftMax...         0.00%       0.000us         0.00%       0.000us       0.000us        1.262s         5.70%        1.262s     430.876us          2929  \n",
      "                                            transformer         0.47%     432.353ms        19.21%       17.738s       6.056ms       0.000us         0.00%        1.252s     427.535us          2929  \n",
      "      autograd::engine::evaluate_function: BmmBackward0         0.21%     197.354ms         1.71%        1.581s     134.908us       0.000us         0.00%        1.125s      96.055us         11716  \n",
      "                                           BmmBackward0         0.22%     202.185ms         1.50%        1.383s     118.063us       0.000us         0.00%        1.125s      96.055us         11716  \n",
      "                                            aten::fill_         0.78%     719.865ms         1.89%        1.746s      19.875us     770.050ms         3.48%     770.050ms       8.766us         87844  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     770.050ms         3.48%     770.050ms       8.766us         87843  \n",
      "                                            aten::zero_         0.70%     641.939ms         2.49%        2.301s      19.648us       0.000us         0.00%     767.038ms       6.548us        117133  \n",
      "                                         self_attention         2.40%        2.214s        10.08%        9.308s       1.589ms       0.000us         0.00%     743.252ms     126.878us          5858  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     740.636ms         3.34%     740.636ms      21.072us         35148  \n",
      "                                             aten::norm         2.74%        2.529s         4.41%        4.075s      49.689us     738.142ms         3.33%     738.142ms       9.000us         82012  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     731.793ms         3.30%     731.793ms       8.923us         82012  \n",
      "autograd::engine::evaluate_function: torch::autograd...         0.78%     724.395ms         3.64%        3.361s      42.505us       0.000us         0.00%     628.076ms       7.942us         79083  \n",
      "                        torch::autograd::AccumulateGrad         0.58%     534.973ms         2.86%        2.637s      33.346us       0.000us         0.00%     628.076ms       7.942us         79083  \n",
      "                                Optimizer.step#SGD.step         0.99%     916.311ms         2.76%        2.546s     869.336us       0.000us         0.00%     610.613ms     208.471us          2929  \n",
      "void at::native::reduce_kernel<128, 4, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     605.097ms         2.73%     605.097ms      25.824us         23432  \n",
      "                                             aten::mul_         0.99%     915.267ms         2.09%        1.931s      24.422us     517.155ms         2.33%     517.155ms       6.539us         79083  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     517.155ms         2.33%     517.155ms       6.539us         79083  \n",
      "                         volta_sgemm_64x32_sliced1x4_nn         0.00%       0.000us         0.00%       0.000us       0.000us     511.554ms         2.31%     511.554ms      29.109us         17574  \n",
      "                                        attn_projection         1.65%        1.528s         8.49%        7.842s       1.339ms       0.000us         0.00%     508.997ms      86.889us          5858  \n",
      "autograd::engine::evaluate_function: NllLossBackward...         0.09%      82.126ms         0.47%     430.371ms     146.934us       0.000us         0.00%     407.874ms     139.254us          2929  \n",
      "                                       NllLossBackward0         0.05%      45.347ms         0.38%     348.245ms     118.896us       0.000us         0.00%     407.874ms     139.254us          2929  \n",
      "                                aten::nll_loss_backward         0.10%      89.961ms         0.33%     302.898ms     103.413us      40.057ms         0.18%     407.874ms     139.254us          2929  \n",
      "                         volta_sgemm_32x32_sliced1x4_nt         0.00%       0.000us         0.00%       0.000us       0.000us     339.347ms         1.53%     339.347ms      19.310us         17574  \n",
      "                                  volta_sgemm_128x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us     334.741ms         1.51%     334.741ms      28.571us         11716  \n",
      "                                  volta_sgemm_128x64_nt         0.00%       0.000us         0.00%       0.000us       0.000us     333.924ms         1.51%     333.924ms      28.502us         11716  \n",
      "autograd::engine::evaluate_function: NativeLayerNorm...         0.23%     214.260ms         1.94%        1.796s     153.285us       0.000us         0.00%     325.303ms      27.766us         11716  \n",
      "                               NativeLayerNormBackward0         0.15%     141.582ms         1.71%        1.582s     134.997us       0.000us         0.00%     325.303ms      27.766us         11716  \n",
      "                       aten::native_layer_norm_backward         0.63%     585.471ms         1.56%        1.440s     122.912us     325.303ms         1.47%     325.303ms      27.766us         11716  \n",
      "                      Optimizer.zero_grad#SGD.zero_grad         0.67%     618.679ms         2.85%        2.628s     897.121us       0.000us         0.00%     281.548ms      96.124us          2929  \n",
      "                         volta_sgemm_32x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us     248.234ms         1.12%     248.234ms      14.125us         17574  \n",
      "autograd::engine::evaluate_function: EmbeddingBackwa...         0.03%      29.793ms         0.37%     340.399ms     116.217us       0.000us         0.00%     230.077ms      78.551us          2929  \n",
      "                                     EmbeddingBackward0         0.02%      19.362ms         0.34%     310.606ms     106.045us       0.000us         0.00%     230.077ms      78.551us          2929  \n",
      "                               aten::embedding_backward         0.04%      36.140ms         0.32%     291.244ms      99.435us       0.000us         0.00%     230.077ms      78.551us          2929  \n",
      "                         aten::embedding_dense_backward         0.07%      69.016ms         0.28%     255.104ms      87.096us     112.404ms         0.51%     230.077ms      78.551us          2929  \n",
      "void at::native::(anonymous namespace)::GammaBetaBac...         0.00%       0.000us         0.00%       0.000us       0.000us     197.848ms         0.89%     197.848ms      16.887us         11716  \n",
      "                                       aten::layer_norm         0.08%      77.019ms         1.95%        1.799s     153.561us       0.000us         0.00%     185.345ms      15.820us         11716  \n",
      "                                aten::native_layer_norm         1.21%        1.120s         1.86%        1.722s     146.987us     185.345ms         0.84%     185.345ms      15.820us         11716  \n",
      "                                            aten::copy_         0.58%     532.953ms         1.25%        1.152s      32.748us     181.026ms         0.82%     181.026ms       5.146us         35177  \n",
      "                                            aten::clone         0.45%     419.593ms         2.29%        2.117s      60.238us       0.000us         0.00%     180.869ms       5.146us         35148  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     180.869ms         0.82%     180.869ms       5.146us         35148  \n",
      "                                  volta_sgemm_128x32_tn         0.00%       0.000us         0.00%       0.000us       0.000us     158.927ms         0.72%     158.927ms      27.130us          5858  \n",
      "void at::native::(anonymous namespace)::RowwiseMomen...         0.00%       0.000us         0.00%       0.000us       0.000us     148.730ms         0.67%     148.730ms      12.695us         11716  \n",
      "void splitKreduce_kernel<float, float, float>(cublas...         0.00%       0.000us         0.00%       0.000us       0.000us     141.500ms         0.64%     141.500ms       4.392us         32219  \n",
      "                                            aten::zeros         0.35%     321.262ms         0.62%     576.894ms      16.413us       0.000us         0.00%     117.673ms       3.348us         35148  \n",
      "     autograd::engine::evaluate_function: ViewBackward0         0.61%     567.824ms         2.70%        2.492s      47.268us       0.000us         0.00%     115.774ms       2.196us         52722  \n",
      "void at::native::(anonymous namespace)::embedding_ba...         0.00%       0.000us         0.00%       0.000us       0.000us     112.404ms         0.51%     112.404ms      38.376us          2929  \n",
      "                                              aten::cat         0.08%      73.505ms         0.64%     590.792ms      67.235us       0.000us         0.00%     110.712ms      12.600us          8787  \n",
      "                                             aten::_cat         0.26%     239.077ms         0.56%     517.287ms      58.870us     110.712ms         0.50%     110.712ms      12.600us          8787  \n",
      "                                       aten::contiguous         0.10%      89.954ms         1.69%        1.559s      66.554us       0.000us         0.00%      96.638ms       4.124us         23432  \n",
      "                                              aten::add         0.74%     685.758ms         1.24%        1.142s      38.984us      88.310ms         0.40%      88.310ms       3.015us         29290  \n",
      "                                          aten::reshape         0.63%     579.179ms         1.77%        1.635s      19.249us       0.000us         0.00%      84.231ms       0.992us         84941  \n",
      "                                          ViewBackward0         0.26%     238.258ms         1.65%        1.528s      28.978us       0.000us         0.00%      84.231ms       1.598us         52722  \n",
      "    autograd::engine::evaluate_function: SplitBackward0         0.08%      74.564ms         0.52%     480.646ms      82.050us       0.000us         0.00%      83.828ms      14.310us          5858  \n",
      "                                         SplitBackward0         0.03%      31.397ms         0.44%     406.082ms      69.321us       0.000us         0.00%      83.828ms      14.310us          5858  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      83.828ms         0.38%      83.828ms      14.310us          5858  \n",
      "                                          aten::dropout         0.19%     174.353ms         3.02%        2.786s     105.689us       0.000us         0.00%      78.232ms       2.968us         26361  \n",
      "                                   aten::_fused_dropout         1.20%        1.112s         2.83%        2.612s      99.075us      78.232ms         0.35%      78.232ms       2.968us         26361  \n",
      "void at::native::(anonymous namespace)::fused_dropou...         0.00%       0.000us         0.00%       0.000us       0.000us      78.232ms         0.35%      78.232ms       2.968us         26361  \n",
      "void at::native::(anonymous namespace)::ComputeInter...         0.00%       0.000us         0.00%       0.000us       0.000us      76.601ms         0.35%      76.601ms       6.538us         11716  \n",
      "                                      aten::nll_loss_nd         0.02%      16.999ms         0.21%     193.130ms      65.937us       0.000us         0.00%      72.021ms      24.589us          2929  \n",
      "                                         aten::nll_loss         0.02%      21.039ms         0.19%     176.131ms      60.133us       0.000us         0.00%      72.021ms      24.589us          2929  \n",
      "                                 aten::nll_loss_forward         0.11%     106.032ms         0.17%     155.092ms      52.950us      72.021ms         0.33%      72.021ms      24.589us          2929  \n",
      "void at::native::(anonymous namespace)::nll_loss_for...         0.00%       0.000us         0.00%       0.000us       0.000us      72.021ms         0.33%      72.021ms      24.589us          2929  \n",
      "autograd::engine::evaluate_function: FusedDropoutBac...         0.32%     293.519ms         1.95%        1.803s      68.407us       0.000us         0.00%      53.958ms       2.047us         26361  \n",
      "                                  FusedDropoutBackward0         0.19%     179.284ms         1.63%        1.510s      57.273us       0.000us         0.00%      53.958ms       2.047us         26361  \n",
      "                                    aten::_masked_scale         0.56%     513.135ms         1.44%        1.330s      50.472us      53.958ms         0.24%      53.958ms       2.047us         26361  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      53.958ms         0.24%      53.958ms       2.047us         26361  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      48.369ms         0.22%      48.369ms       1.835us         26361  \n",
      "     autograd::engine::evaluate_function: GeluBackward0         0.09%      78.704ms         0.34%     311.074ms      53.102us       0.000us         0.00%      43.758ms       7.470us          5858  \n",
      "                                          GeluBackward0         0.04%      38.043ms         0.25%     232.370ms      39.667us       0.000us         0.00%      43.758ms       7.470us          5858  \n",
      "                                    aten::gelu_backward         0.12%     110.550ms         0.21%     194.327ms      33.173us      43.758ms         0.20%      43.758ms       7.470us          5858  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      43.758ms         0.20%      43.758ms       7.470us          5858  \n",
      "void at::native::(anonymous namespace)::nll_loss_bac...         0.00%       0.000us         0.00%       0.000us       0.000us      40.057ms         0.18%      40.057ms      13.676us          2929  \n",
      "void at::native::(anonymous namespace)::LayerNormFor...         0.00%       0.000us         0.00%       0.000us       0.000us      36.615ms         0.17%      36.615ms       3.125us         11716  \n",
      "void at::native::(anonymous namespace)::LayerNormBac...         0.00%       0.000us         0.00%       0.000us       0.000us      35.986ms         0.16%      35.986ms       3.072us         11716  \n",
      "                                    positional_encoding         0.29%     263.929ms         0.82%     760.053ms     259.492us       0.000us         0.00%      34.404ms      11.746us          2929  \n",
      "                                              aten::div         0.55%     503.910ms         0.86%     790.009ms      44.953us      32.814ms         0.15%      32.814ms       1.867us         17574  \n",
      "                                          aten::softmax         0.05%      45.433ms         0.32%     295.995ms      50.528us       0.000us         0.00%      29.449ms       5.027us          5858  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 92.358s\n",
      "Self CUDA time total: 22.153s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(stat.table(sort_by=\"cuda_time_total\", row_limit=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00eef62-76ce-4a31-a48b-6b61c4b68f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21c5d4-8599-4f11-a925-a9c3a2b21ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff56493-ffae-4c72-a7f1-bb20ea1e2c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "123\n",
      "15129\n",
      "1860867\n",
      "228886641\n",
      "28153056843\n",
      "3462825991689\n",
      "425927596977747\n",
      "52389094428262881\n",
      "6443858614676334363\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(123 ** i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6d378-0cf5-4440-91bc-3acb83e355b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d757fa1-4897-4c0d-b2d9-479d7f3a612a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(45**67) % 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1016e3b-792b-4129-9541-56eb46a940d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c555bc-8230-45f5-904d-8468d25c3399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1b2c211-149b-4fb7-87fb-2391b1902d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62656a3c-9fee-4977-85d2-9a49692391ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.645751311064591"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3+math.sqrt(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fccf416-7dfa-4212-8445-06b0267f90e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179.95554457618744"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5.6457513110645 ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a184c90b-f698-48a2-b6b2-355a507a57c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179.9555445761961"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5.645751311064591 ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8020ae2a-4469-4cb0-b83b-9c7c231a0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = np.zeros(200, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7c38c9e-c491-4a56-94cc-612ef6fcfb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "581129af-30ee-4f8c-9524-49b9f799922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 101):\n",
    "    dp[i] = dp[i-1]\n",
    "    if i >= 3:\n",
    "        dp[i] += dp[i-3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1cd4052-72d9-4de6-8ade-5e4701f0df07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24382819596721629"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(dp[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad2e54b-462f-4e71-bb0c-8eab2d16732c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830daf0-7068-49a8-b224-a28022a579a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
